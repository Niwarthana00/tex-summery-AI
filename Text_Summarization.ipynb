{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Text Summarization with T5\n",
                "\n",
                "This notebook covers the end-to-end process of fine-tuning a T5-small model on the CNN/DailyMail dataset for text summarization."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install transformers datasets rouge_score py7zr accelerate evaluate nltk pandas matplotlib"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import nltk\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import evaluate\n",
                "from datasets import load_dataset\n",
                "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
                "\n",
                "nltk.download('punkt')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Dataset Collection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the CNN/DailyMail dataset\n",
                "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
                "\n",
                "# Optimization: Use a smaller subset for faster training\n",
                "print(\"Slicing dataset for faster training...\")\n",
                "dataset[\"train\"] = dataset[\"train\"].select(range(2000))\n",
                "dataset[\"validation\"] = dataset[\"validation\"].select(range(500))\n",
                "\n",
                "print(f\"Train: {len(dataset['train'])}\")\n",
                "print(f\"Validation: {len(dataset['validation'])}\")\n",
                "print(f\"Test: {len(dataset['test'])}\")\n",
                "\n",
                "# Show a sample\n",
                "sample = dataset['train'][0]\n",
                "print(f\"\\nArticle:\\n{sample['article'][:500]}...\")\n",
                "print(f\"\\nHighlights:\\n{sample['highlights']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Exploration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analyze token lengths to determine max_input_length and max_target_length\n",
                "# Taking a subset for speed if needed, but here's the logic\n",
                "\n",
                "train_df = pd.DataFrame(dataset['train'].select(range(1000))) # select first 1000 for quick analysis\n",
                "\n",
                "train_df['article_len'] = train_df['article'].apply(lambda x: len(x.split()))\n",
                "train_df['highlights_len'] = train_df['highlights'].apply(lambda x: len(x.split()))\n",
                "\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
                "ax1.hist(train_df['article_len'], bins=50)\n",
                "ax1.set_title('Article Word Count')\n",
                "ax2.hist(train_df['highlights_len'], bins=50)\n",
                "ax2.set_title('Summary Word Count')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_checkpoint = \"t5-small\"\n",
                "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
                "\n",
                "max_input_length = 1024\n",
                "max_target_length = 128\n",
                "prefix = \"summarize: \"\n",
                "\n",
                "def preprocess_function(examples):\n",
                "    inputs = [prefix + doc for doc in examples[\"article\"]]\n",
                "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
                "\n",
                "    # Setup the tokenizer for targets\n",
                "    with tokenizer.as_target_tokenizer():\n",
                "        labels = tokenizer(examples[\"highlights\"], max_length=max_target_length, truncation=True)\n",
                "\n",
                "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
                "    return model_inputs\n",
                "\n",
                "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Model Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
                "batch_size = 16\n",
                "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Model Training"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "args = Seq2SeqTrainingArguments(\n",
                "    f\"{model_checkpoint}-finetuned-cnn\",\n",
                "    eval_strategy = \"epoch\",\n",
                "    learning_rate=2e-5,\n",
                "    per_device_train_batch_size=batch_size,\n",
                "    per_device_eval_batch_size=batch_size,\n",
                "    weight_decay=0.01,\n",
                "    save_total_limit=3,\n",
                "    num_train_epochs=3,\n",
                "    predict_with_generate=True,\n",
                "    fp16=True, # Enable mixed precision for faster training on GPU\n",
                ")\n",
                "\n",
                "# Metric setup\n",
                "metric = evaluate.load(\"rouge\")\n",
                "\n",
                "def compute_metrics(eval_pred):\n",
                "    predictions, labels = eval_pred\n",
                "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
                "    # Replace -100 in the labels as we can't decode them.\n",
                "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
                "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
                "    \n",
                "    # Rouge expects a newline after each sentence\n",
                "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
                "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
                "    \n",
                "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
                "    # Extract a few results\n",
                "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
                "    return {k: round(v, 4) for k, v in result.items()}\n",
                "\n",
                "trainer = Seq2SeqTrainer(\n",
                "    model,\n",
                "    args,\n",
                "    train_dataset=tokenized_datasets[\"train\"],\n",
                "    eval_dataset=tokenized_datasets[\"validation\"],\n",
                "    data_collator=data_collator,\n",
                "    tokenizer=tokenizer,\n",
                "    compute_metrics=compute_metrics\n",
                ")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.train()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Model Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.evaluate()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "trainer.save_model(\"./my_summarization_model\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}